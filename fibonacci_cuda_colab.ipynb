{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "3Esok5YqIYhe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HhNJ-HqAEqrm"
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba==0.57.0  # Try a slightly older version\n",
        "!pip install llvmlite==0.40.0  # Numba often has a corresponding llvmlite version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfOxhOcjSgwi",
        "outputId": "276d806f-15de-4444-b123-a58ef1fc6c59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba==0.57.0 in /usr/local/lib/python3.11/dist-packages (0.57.0)\n",
            "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.57.0) (0.40.0)\n",
            "Requirement already satisfied: numpy<1.25,>=1.21 in /usr/local/lib/python3.11/dist-packages (from numba==0.57.0) (1.24.4)\n",
            "Requirement already satisfied: llvmlite==0.40.0 in /usr/local/lib/python3.11/dist-packages (0.40.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CUDA Kernel**"
      ],
      "metadata": {
        "id": "cfKpCKsqIvBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def fibonacci_cuda_kernel(out):\n",
        "    \"\"\"CUDA kernel to compute the Fibonacci sequence.\"\"\"\n",
        "    idx = cuda.grid(1)\n",
        "    if idx < out.shape[0]:\n",
        "        if idx == 0:\n",
        "            out[idx] = 0\n",
        "        elif idx == 1:\n",
        "            out[idx] = 1\n",
        "        else:\n",
        "            out[idx] = out[idx - 1] + out[idx - 2]\n",
        "\n",
        "def fibonacci_cuda(n):\n",
        "    \"\"\"Launches the CUDA kernel to compute the Fibonacci sequence.\"\"\"\n",
        "    if n <= 0:\n",
        "        return np.array([])\n",
        "    out_device = cuda.device_array(n, dtype=np.int64)\n",
        "    threadsperblock = 256\n",
        "    blockspergrid = (n + (threadsperblock - 1)) // threadsperblock\n",
        "    fibonacci_cuda_kernel[blockspergrid, threadsperblock](out_device)\n",
        "    return out_device.copy_to_host()"
      ],
      "metadata": {
        "id": "uE2gG3umHneX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequential Implementation**"
      ],
      "metadata": {
        "id": "YVFePYiaI2mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fibonacci_cuda(n):\n",
        "    \"\"\"Launches the CUDA kernel to compute the Fibonacci sequence.\"\"\"\n",
        "    if n <= 0:\n",
        "        return np.array([])\n",
        "    out_device = cuda.device_array(n, dtype=np.int64)\n",
        "    threadsperblock = 256\n",
        "    blockspergrid = (n + (threadsperblock - 1)) // threadsperblock\n",
        "    fibonacci_cuda_kernel[blockspergrid, threadsperblock](out_device)\n",
        "    return out_device.copy_to_host()\n",
        "\n"
      ],
      "metadata": {
        "id": "dDpGeRpzHu5r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Sequential Implementation ---\n",
        "def fibonacci_sequential(n):\n",
        "    \"\"\"Computes the Fibonacci sequence sequentially.\"\"\"\n",
        "    if n <= 0:\n",
        "        return []\n",
        "    elif n == 1:\n",
        "        return [0]\n",
        "    else:\n",
        "        list_fib = [0, 1]\n",
        "        while len(list_fib) < n:\n",
        "            next_fib = list_fib[-1] + list_fib[-2]\n",
        "            list_fib.append(next_fib)\n",
        "        return list_fib\n",
        "\n"
      ],
      "metadata": {
        "id": "ntTAslU7Hzrc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Execution and Comparison**"
      ],
      "metadata": {
        "id": "qn0C1UpKI_ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution and Comparison ---\n",
        "if __name__ == \"__main__\":\n",
        "    N = 220\n",
        "\n",
        "    # Sequential Execution\n",
        "    start_time_seq = time.time()\n",
        "    fib_seq = fibonacci_sequential(N)\n",
        "    end_time_seq = time.time()\n",
        "    sequential_time = end_time_seq - start_time_seq\n",
        "    print(f\"Sequential Fibonacci ({N} numbers) took: {sequential_time:.6f} seconds\")\n",
        "\n",
        "    # CUDA Execution\n",
        "    try:\n",
        "        start_time_cuda = time.time()\n",
        "        fib_cuda = fibonacci_cuda(N)\n",
        "        end_time_cuda = time.time()\n",
        "        cuda_time = end_time_cuda - start_time_cuda\n",
        "        print(f\"CUDA Fibonacci ({N} numbers) took: {cuda_time:.6f} seconds\")\n",
        "        speedup = sequential_time / cuda_time\n",
        "        print(f\"\\nSpeedup (Sequential / CUDA): {speedup:.2f}x\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCUDA Error: {e}\")\n",
        "        print(\"Make sure you have a CUDA-enabled GPU and the necessary drivers installed and compatible.\")\n",
        "        print(\"Falling back to CPU execution for comparison.\")\n",
        "        fib_cuda = fib_seq\n",
        "        speedup = 1.0\n",
        "\n",
        "    # Verification (optional)\n",
        "    if N <= 20:\n",
        "        if np.array_equal(fib_seq, fib_cuda):\n",
        "            print(\"\\nResults from sequential and CUDA implementations match.\")\n",
        "        else:\n",
        "            print(\"\\nResults from sequential and CUDA implementations DO NOT match!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr7y7flVH35Q",
        "outputId": "4da6c7e8-756c-4a27-c01a-b8a57bb0fdc3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential Fibonacci (220 numbers) took: 0.000036 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "ERROR:numba.cuda.cudadrv.driver:Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CUDA Error: [222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION\n",
            "ptxas application ptx input, line 9; fatal   : Unsupported .version 8.5; current version is '8.4'\n",
            "Make sure you have a CUDA-enabled GPU and the necessary drivers installed and compatible.\n",
            "Falling back to CPU execution for comparison.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --- Observation ---\n",
        "\n",
        "For a relatively small number of elements like N = 220, the overhead of transferring data to and from the GPU, as well as the kernel launch and grid/block setup, can often outweigh the benefits of parallel computation. In this scenario, you might observe that the sequential implementation is faster or has a similar execution time to the CUDA implementation.\n",
        "\n",
        "However, as the value of N increases significantly, the parallel nature of the CUDA kernel *could* become more advantageous if the PTX compatibility issues are resolved. The current CUDA implementation has dependencies between threads, limiting parallelism. More advanced CUDA techniques would be needed for better GPU utilization.\n",
        "\n",
        "**Important Note:** This code uses older versions of Numba and llvmlite."
      ],
      "metadata": {
        "id": "N9rLQP0VJTSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TcH8AJ1IZsi",
        "outputId": "69924f8e-ac7d-40b7-ce0b-05d379384eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "km92aVYhMzZC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}